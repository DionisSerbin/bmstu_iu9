{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "FILE_NAME = 'model.json'\n",
    "\n",
    "\n",
    "class Sample:\n",
    "    def __init__(self, input, expected):\n",
    "        self.input = input\n",
    "        self.expected = expected\n",
    "\n",
    "    def show(self):\n",
    "        plt.imshow(self.input.reshape(28, 28), cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def combine_samples(samples):\n",
    "    inpt = np.array([x.input for x in samples]).transpose()\n",
    "    expect = np.array([x.expected for x in samples]).transpose()\n",
    "    return Sample(inpt, expect)\n",
    "\n",
    "\n",
    "def get_samples(train):\n",
    "    samples = []\n",
    "    x_train, t_train, x_test, t_test = mnist_load()\n",
    "    if train:\n",
    "        data = zip(x_train, t_train)\n",
    "    else:\n",
    "        data = zip(x_test, t_test)\n",
    "    for x, y in data:\n",
    "        one_hot = np.array([int(i == y) for i in range(10)])\n",
    "        samples.append(Sample(np.vectorize(lambda x : x / 255)(x), one_hot))\n",
    "    return samples\n",
    "\n",
    "\n",
    "def mnist_load():\n",
    "    with open(\"mnist/mnist.pkl\", \"rb\") as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return (\n",
    "        mnist[\"training_images\"],\n",
    "        mnist[\"training_labels\"],\n",
    "        mnist[\"test_images\"],\n",
    "        mnist[\"test_labels\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def show_random_wrong_sample(score):\n",
    "    wrong = []\n",
    "    for pred in score.predictions:\n",
    "        if pred.sample.expected.argmax() != pred.prediction.argmax():\n",
    "            wrong.append(pred)\n",
    "    w = wrong[random.randint(0, len(wrong))]\n",
    "    print('Expected: ', w.sample.expected.argmax())\n",
    "    print('Got: ', w.prediction.argmax())\n",
    "    w.sample.show()\n",
    "\n",
    "\n",
    "class Prediction:\n",
    "    def __init__(self, sample, prediction):\n",
    "        self.sample = sample\n",
    "        self.prediction = prediction\n",
    "\n",
    "\n",
    "class ScoreResult:\n",
    "    def __init__(self, accuracy, loss, predictions):\n",
    "        self.accuracy = accuracy\n",
    "        self.loss = loss\n",
    "        self.predictions = predictions\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"accuracy: {self.accuracy * 100}%\\n loss: {self.loss}\"\n",
    "\n",
    "\n",
    "class RunResult:\n",
    "    def __init__(self, activations, weighted_inputs):\n",
    "        self.activations = activations\n",
    "        self.weighted_inputs = weighted_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train_data = get_samples(True)\n",
    "test_data = get_samples(False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def init_params(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def step(self, grad_w, grad_b, batch):\n",
    "        for i in range(1, len(self.model.layers)):\n",
    "            self.model.weights[i] -= self.lr * grad_w[i]\n",
    "            self.model.biases[i] -= self.lr * grad_b[i]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, layers, optimizer, batch_size, l1=0, l2=0, dropout_rate=0):\n",
    "        self.current_epoch = 0\n",
    "        self.layers = layers\n",
    "        self.weights = [None]\n",
    "        self.biases = [None]\n",
    "        self.batch_size = batch_size\n",
    "        for i in range(1, len(layers)):\n",
    "            self.weights.append(np.random.uniform(-1, -1, (layers[i], layers[i - 1])) * np.sqrt(1/layers[i - 1]))\n",
    "            self.biases.append(np.random.uniform(-1, 1, layers[i]))\n",
    "        self.optimizer = optimizer\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dropout_masks = [None]\n",
    "        optimizer.init_params(self)\n",
    "\n",
    "    def update_dropout_masks(self):\n",
    "        self.dropout_masks = [None]\n",
    "        for layer in self.layers[1:]:\n",
    "            v = np.random.rand(layer) > self.dropout_rate\n",
    "            self.dropout_masks.append(v)\n",
    "\n",
    "    def dump(self, path):\n",
    "        d = {\n",
    "            \"layers\": self.layers,\n",
    "            \"weights\": [x.tolist() for x in self.weights[1:]],\n",
    "            \"biases\": [x.tolist() for x in self.biases[1:]],\n",
    "        }\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(d, f)\n",
    "\n",
    "    def load(self, path):\n",
    "        with open(path, 'r') as f:\n",
    "            d = json.load(f)\n",
    "        self.layers = d[\"layers\"]\n",
    "        self.weights = [None] + [np.array(x) for x in d[\"weights\"]]\n",
    "        self.biases = [None] + [np.array(x) for x in d[\"biases\"]]\n",
    "\n",
    "    def train(self, samples, epochs, monitor_dataset=None):\n",
    "        samples_copy = samples[:]\n",
    "        res = []\n",
    "        for epoch in range(epochs):\n",
    "            self.current_epoch += 1\n",
    "            random.shuffle(samples_copy)\n",
    "            for i in range(0, len(samples_copy), self.batch_size):\n",
    "                self.update_dropout_masks()\n",
    "                batch = combine_samples(samples_copy[i:i+self.batch_size])\n",
    "                result = self.run(batch, True)\n",
    "                errors = self.backprop(batch, result)\n",
    "                grad_w, grad_b = self.calculate_grad(result.activations, errors)\n",
    "                self.optimizer.step(grad_w, grad_b, batch)\n",
    "            if monitor_dataset is not None:\n",
    "                score = self.score(monitor_dataset)\n",
    "                res.append(score)\n",
    "        return res\n",
    "\n",
    "    def run(self, sample, use_dropout):\n",
    "        activations = [None for _ in self.layers]\n",
    "        activations[0] = sample.input\n",
    "        weighted_inputs = [None for _ in self.layers]\n",
    "        for i in range(1, len(self.layers)):\n",
    "            weighted_inputs[i] = self.weights[i] @ activations[i - 1] + self.biases[i][:,np.newaxis]\n",
    "            activations[i] = self.calc_activation(weighted_inputs[i])\n",
    "            if use_dropout and i + 1 != len(self.layers):\n",
    "                weighted_inputs[i] *= self.dropout_masks[i][:,np.newaxis]\n",
    "                activations[i] *= self.dropout_masks[i][:,np.newaxis]\n",
    "                activations[i] /= 1 - self.dropout_rate\n",
    "        return RunResult(activations, weighted_inputs)\n",
    "\n",
    "    def backprop(self, sample, result):\n",
    "        errors = [None for _ in self.layers]\n",
    "        errors[-1] = self.calc_delta(sample.expected, result.activations[-1])\n",
    "        for i in reversed(range(1, len(self.layers) - 1)):\n",
    "            errors[i] = (\n",
    "                (np.transpose(self.weights[i + 1]) @ errors[i + 1])\n",
    "                * self.calc_activation_derivative(result.weighted_inputs[i])\n",
    "            ) * self.dropout_masks[i][:,np.newaxis]\n",
    "            errors[i] /= 1 - self.dropout_rate\n",
    "        return errors\n",
    "\n",
    "    def calculate_grad(self, activations, errors):\n",
    "        grad_w = [None]\n",
    "        grad_b = [None]\n",
    "        for i in range(1, len(self.layers)):\n",
    "            w = errors[i] @ activations[i - 1].transpose()\n",
    "            w += self.l1 * np.sign(self.weights[i]) + self.l2 * self.weights[i]\n",
    "            b = errors[i].sum(axis=1) / self.batch_size\n",
    "            if i + 1 != len(self.layers):\n",
    "                w *= self.dropout_masks[i][:,np.newaxis]\n",
    "                b *= self.dropout_masks[i]\n",
    "            grad_w.append(w / self.batch_size)\n",
    "            grad_b.append(b / self.batch_size)\n",
    "        return grad_w, grad_b\n",
    "\n",
    "    def score(self, samples):\n",
    "        cost = 0\n",
    "        accurate = 0\n",
    "        predictions = []\n",
    "        for sample in samples:\n",
    "            res = self.run(combine_samples([sample]), False)\n",
    "            out = res.activations[-1][:, 0]\n",
    "            pred = Prediction(sample, out)\n",
    "            predictions.append(pred)\n",
    "            cost += self.calc_cost(sample.expected, out)\n",
    "            if out.argmax() == sample.expected.argmax():\n",
    "                accurate += 1\n",
    "        return ScoreResult(accurate / len(samples), cost / len(samples), predictions)\n",
    "\n",
    "    def calc_cost(self, expected, out):\n",
    "        return np.sum(np.nan_to_num(-expected*np.log(out)) - (1 - expected) * np.log(1 - out))\n",
    "\n",
    "    def calc_delta(self, expected, activations):\n",
    "        return (activations - expected)\n",
    "\n",
    "    def calc_activation(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def calc_activation_derivative(self, x):\n",
    "        return self.calc_activation(x) * (1 - self.calc_activation(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Capacity: 20.0%\n",
      " accuracy: 90.10000000000001%\n",
      " loss: 0.689685522461967\n",
      "\n",
      "Capacity: 40.0%\n",
      " accuracy: 89.67%\n",
      " loss: 0.7009716116041779\n",
      "\n",
      "Capacity: 60.0%\n",
      " accuracy: 89.53%\n",
      " loss: 0.6731876981035096\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "PERCENTEGES = [0.2, 0.4, 0.6, 0.8, 1]\n",
    "scores = []\n",
    "for p in PERCENTEGES:\n",
    "    network = Network([784, 10, 10], optimizer=SGD(1), batch_size=10)\n",
    "    samples = int(p * len(train_data))\n",
    "    score = network.train(train_data[:samples], EPOCHS, monitor_dataset=test_data)\n",
    "    print(f\"\\nCapacity: {p * 100}%\\n {score[-1]}\")\n",
    "    scores.append(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xs = list(range(1, EPOCHS + 1))\n",
    "for i, p in enumerate(PERCENTEGES):\n",
    "    ys = [x.loss for x in scores[i]]\n",
    "    plt.plot(xs, ys)\n",
    "labels = [f\"{p * 100}%\" for p in PERCENTEGES]\n",
    "plt.legend(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xs = list(range(1, EPOCHS + 1))\n",
    "for i, p in enumerate(PERCENTEGES):\n",
    "    ys = [x.accuracy for x in scores[i]]\n",
    "    plt.plot(xs, ys)\n",
    "labels = [f\"{p * 100}%\" for p in PERCENTEGES]\n",
    "plt.legend(labels)\n",
    "EPOCHS = 50\n",
    "scores = []\n",
    "for i in range(5):\n",
    "    network = Network([784] + [5] * i + [10], optimizer=SGD(0.1), batch_size=10)\n",
    "    score = network.train(train_data, EPOCHS, monitor_dataset=test_data)\n",
    "    print(f\"\\nHidden layers: {5*i}\\n {score[-1]}\")\n",
    "    scores.append(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xs = list(range(1, EPOCHS + 1))\n",
    "for i in range(5):\n",
    "    ys = [x.loss for x in scores[i]]\n",
    "    plt.plot(xs, ys)\n",
    "labels = [str(i) for i in range(5)]\n",
    "plt.legend(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xs = list(range(1, EPOCHS + 1))\n",
    "for i in range(5):\n",
    "    ys = [x.accuracy for x in scores[i]]\n",
    "    plt.plot(xs, ys)\n",
    "labels = [str(i) for i in range(5)]\n",
    "plt.legend(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params = [\n",
    "    (0, 0, 0),\n",
    "    (0.001, 0, 0),\n",
    "    (0, 0.001, 0),\n",
    "    (0.0005, 0.0005, 0),\n",
    "]\n",
    "labels = [\"SGD\", \"L1\", \"L2\", \"L1+L2\"]\n",
    "scores = []\n",
    "for i, p in enumerate(params):\n",
    "    network = Network([784, 10, 10], SGD(0.1), 10, *p)\n",
    "    score = network.train(train_data, 50, monitor_dataset=test_data)\n",
    "    print(f\"\\n{labels[i]}\\n {score[-1]}\")\n",
    "    scores.append(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xs = list(range(1, 51))\n",
    "for i in range(4):\n",
    "    ys = [x.loss for x in scores[i]]\n",
    "    plt.plot(xs, ys)\n",
    "plt.legend(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xs = list(range(1, 51))\n",
    "for i in range(4):\n",
    "    ys = [x.accuracy for x in scores[i]]\n",
    "    plt.plot(xs, ys)\n",
    "plt.legend(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}